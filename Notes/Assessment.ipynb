{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ways to assess machine learning model\r\n",
    "\r\n",
    "For simplicity, it is dicussing in term of a binary classification problem: We need to find if an image is of a cat or dog. Some common terms are:\r\n",
    "\r\n",
    "True positives (**TF**): Predict positive and are actually positive.\r\n",
    "\r\n",
    "False positives (**FP**): Predict positive and are actually negative.\r\n",
    "\r\n",
    "True negatives (**TN**): Predicted negative and are actually negative.\r\n",
    "\r\n",
    "False negatives (**FN**): Predicted negative and are actually positive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix\r\n",
    "\r\n",
    "![Visualizing the previous term](https://miro.medium.com/max/911/1*yYctsCAlkQHixEHYy0dHPw.png)\r\n",
    "\r\n",
    "We want to concern about the True Positives and True Negative, penalize when the other way happen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracy\r\n",
    "\r\n",
    "Most commonly used metric to judge a model and is actually not a clear indicator of the performance. The worse happens when classes are imbalanced.\r\n",
    "\r\n",
    "$$\\dfrac{TP + TN}{TP + FP + TN + FN}$$\r\n",
    "\r\n",
    "Take the cancer detection model. Chances of having actual cancer is pretty low, let's say 10 out of a hundred. We don't want to miss on a patient who is having cancer but unable to detect (FN). Detecting everyone not having cancer gives an accuracy of 90% straight. The model did nothing here but just gave cancer free for all the 100 predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precision\r\n",
    "Percentage of positive instances out of the total predicted positive instances. Denominator is the model prediction done as positive from the whole given dataset. Take it as to find out 'how much the model is right when it syas it is right'.\r\n",
    "\r\n",
    "$$\\dfrac{TP}{TP + FP}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall / Sensitivity / True Positive Rate\r\n",
    "Percentage of positive instances out of the total actual positive instances present in the dataset. Take it as to find out 'how much extra right ones, the model missed when it showed the right ones'.\r\n",
    "$$\\dfrac{TP}{TP + FN}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specificity\r\n",
    "Percentage of negative instances out of the total actual negative instances. Therefore denominator (TN + FP) here is the actual number of negative instances present in the dataset. It is similar to recall but the shift is on the negative instances. Like finding out how many healthy patients were not having cancer and were told they donâ€™t have cancer. Kind of a measure to see how separate the classes are.\r\n",
    "$$\\dfrac{TN}{TN + FP}$$"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('cs116': conda)"
  },
  "interpreter": {
   "hash": "ade8686335d92f1830b6286b5bb03bb8a9ebf5a6fa66663edb2ae741c0119b75"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}