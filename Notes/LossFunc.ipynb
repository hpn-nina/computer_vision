{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "## Usage\n",
    "To quantify how good or bad the model is performing.\\\n",
    "Divided into 2 categories i.e.Regression loss and Classification Loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression loss\n",
    "### Mean Square Error\n",
    "\n",
    "MSE is the mean of squared differences between the actual and predicted value. If the difference is large, the model will penalized it as we are computing the squared difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression MSE pratical implement\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=5000, n_features=20, noise=0.1, random_state=1)\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "# split into train and test\n",
    "train1 = 2500\n",
    "trainX, testX = X[:train1, :], X[train1:, :]\n",
    "trainy, testy = y[:train1], y[train1:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Logarithmic Error Loss\n",
    "\n",
    "This model will penalize less than MSE. Take the log of the predicted variable than take MSE. Can overcome the problem possessed by the MSE method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of MSLE\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "# plot loss during training\n",
    "pyplot.title('Mean Squared Logarithmic Error Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error Loss\n",
    "When there are allienated data points, using MAEL will be used to calculates the average of the absolute difference between the actual and predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "# plot loss during training\n",
    "pyplot.title('Mean Absolute Error Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Loss Function\n",
    "\n",
    "Used in scenerio we are dealing with a yes/no question\n",
    "\n",
    "### Binary Cross Entropy Loss\n",
    "It gives the probability value between 0 and 1 for a classification task. Cross-Entropy calculates the average difference between the predicted and actual probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=5000, noise=0.1, random_state=1)\n",
    "# split into train and test\n",
    "train1 = 2500\n",
    "trainX, testX = X[:train1, :], X[train1:, :]\n",
    "trainy, testy = y[:train1], y[train1:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "# plot loss during training\n",
    "pyplot.title('Binary Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss\n",
    "\n",
    "This type of loss is used when the target variable has 1 or -1 as class labels. It penalizes the model when there is a difference in the sign between the actual and predicted class values.\\\n",
    "Ususally used in SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge Loss\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "# plot loss during training\n",
    "pyplot.title('Hinge Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification Loss Function\n",
    "\n",
    "In case we need to predict three-or-more class labels.\n",
    "\n",
    "### Categorical Cross Entropy Loss\n",
    "\n",
    "These are similar to binary classification cross-entropy, used for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class Cross-entropy loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=5000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "train1 = 500\n",
    "trainX, testX = X[:train1, :], X[train1:, :]\n",
    "trainy, testy = y[:train1], y[train1:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "# plot loss during training\n",
    "pyplot.title('Categorical Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback Leiber Divergence Loss\n",
    "It calculates how much a given distribution is away from the true distribution. These are used to carry out complex operations like autoencoder where there is a need to learn the dense feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "# plot loss during training\n",
    "pyplot.title('KL Divergence Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CNN and clasification\n",
    "They would use binary cross-entropy loss\n",
    "- Only tell yes / no\n",
    "- Use log e not log base 10\n",
    "- Realate to convex or non-convex\n",
    "- Log e only decrease from 1 to 0\n",
    "\n",
    "Categorical cross entropy loss\n",
    "- Basically the same with binary one with extended form\n",
    "\n",
    "If there is unbalanced in dataset that has more -> Then cross entropy loss will have bias effect\n",
    "-> Apply weighted class Cross-entropy loss\n",
    "-> Optimize params with less data\n",
    "\n",
    "Can use augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Focal loss](./FL.png)\n",
    "Focal Loss\n",
    "\n",
    "Choose gamma from the beginning\n",
    "Fl when $\\gamma = 0$ is CE\n",
    "\n",
    "- Deal with unbalanced dataset\n",
    "- With the one that have more data, it has more well-classified\n",
    "- The bigger the class the bigger the gamme we give (Make it stop optimize soon)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade8686335d92f1830b6286b5bb03bb8a9ebf5a6fa66663edb2ae741c0119b75"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cs116': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
